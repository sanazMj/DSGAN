{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_baseline_transferlearning_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPzcomX9jP5XjPFY2bcfD2M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanazMj/DSGAN/blob/master/Train_baseline_transferlearning_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOJ-qvndfUCX"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXDtaWG3HZWN"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.applications import ResNet50, DenseNet121, VGG16\n",
        "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from skimage.transform import resize\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import glob\n",
        "import copy\n",
        "import random"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfQcxP3OSHlC",
        "outputId": "a83c3221-c4c2-4d71-fd6d-0f7ed74f3ff5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnyctG6iSLJJ"
      },
      "source": [
        "Project_path =  \"/content/drive/MyDrive/Alzheimer_research/\"\n",
        "data_path = Project_path + '/Sanaz_Code/OASIS_data/New/'"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o60dXVemN64J"
      },
      "source": [
        "img_width, img_height = 224, 224\n",
        "fold_nums = 5\n",
        "batch_num = 30\n",
        "num_epochs = 50\n",
        "num_repeats = 1\n",
        "early_stopping_patience = 5\n",
        "model_cats = ['Baseline', 'Transfer_learning', 'Fine_tuning']\n",
        "model_kinds = [VGG16, ResNet50, DenseNet121]\n",
        "kfold = KFold(n_splits=fold_nums, shuffle=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngjV7qqIN8cg"
      },
      "source": [
        "# Load OASIS data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3rPRMD9rQgj"
      },
      "source": [
        "# data_path = Project_path + '/Sanaz_Code/OASIS_data/'\n",
        "# Alzheimers_files = []\n",
        "# NonAlzheimers_files = []\n",
        "\n",
        "# counter = 0\n",
        "# path, dirs, files = next(os.walk(data_path + '/Alzheimers/'))\n",
        "# for d in dirs:\n",
        "#   path, dirs, files = next(os.walk(data_path + '/Alzheimers/' + d))\n",
        "#   for f in files:\n",
        "#     counter+=1\n",
        "#     file_select = data_path + 'Alzheimers/' + d +'/'+ f\n",
        "#     print(counter)\n",
        "#     os.rename(src = file_select, dst = data_path + '/New/'+'/Alzheimers/' + d + '_' + f )\n",
        "\n",
        "\n",
        "# counter = 0\n",
        "# path, dirs, files = next(os.walk(data_path + '/NonAlzheimer/'))\n",
        "# for d in dirs:\n",
        "#   path, dirs, files = next(os.walk(data_path + '/NonAlzheimer/' + d))\n",
        "#   for f in files:\n",
        "#     counter+=1\n",
        "#     file_select = data_path + 'NonAlzheimer/' + d +'/'+ f\n",
        "#     print(counter)\n",
        "#     os.rename(src = file_select, dst = data_path + '/New/'+'/NonAlzheimer/' + d + '_' + f )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wJwzpjmqjUM"
      },
      "source": [
        "data_path = Project_path + '/Sanaz_Code/OASIS_data/New/'\n",
        "Alzheimers_files = []\n",
        "NonAlzheimers_files = []\n",
        "\n",
        "path, dirs, files = next(os.walk(data_path +'/Alzheimers/'))\n",
        "for f in files:\n",
        "  Alzheimers_files += files\n",
        "\n",
        "path, dirs, files = next(os.walk(data_path +'/NonAlzheimer/'))\n",
        "for f in files: \n",
        "  NonAlzheimers_files += files"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIR08dZ1Q8yq"
      },
      "source": [
        "def get_data(num_folds, len_AD_class = 3200):\n",
        "    \n",
        "\n",
        "  train_len =  int(0.64 * len_AD_class)\n",
        "  val_len =  int(0.16 * len_AD_class)\n",
        "  test_len = int(0.20 * len_AD_class)\n",
        "\n",
        "  y_AD = []\n",
        "  X_AD = []\n",
        "  y_NonAD = []\n",
        "  X_NonAD = []\n",
        "\n",
        "  for filename in os.listdir(Project_path + '/Sanaz_Code/OASIS_data/New/Alzheimers/'):\n",
        "\n",
        "      temp = Image.open(Project_path + '/Sanaz_Code/OASIS_data/New/Alzheimers/' + filename)\n",
        "      temp = temp.resize((img_width, img_height),Image.ANTIALIAS)\n",
        "      temp = np.asarray(temp)\n",
        "      RGB_X = np.stack((temp,)*3, axis = -1)\n",
        "      # print(RGB_X.shape)\n",
        "\n",
        "      X_AD.append(RGB_X)\n",
        "\n",
        "      if len(X_AD) % 200 == 0:\n",
        "        print(len(X_AD), \" of the 3200 have been uploaded\")\n",
        "  for filename in os.listdir(Project_path + '/Sanaz_Code/OASIS_data/New/NonAlzheimer/'):\n",
        "\n",
        "      temp = Image.open(Project_path + '/Sanaz_Code/OASIS_data/New/NonAlzheimer/' + filename)\n",
        "      temp = temp.resize((img_width, img_height),Image.ANTIALIAS)\n",
        "      temp = np.asarray(temp)\n",
        "      RGB_X = np.stack((temp,)*3, axis = -1)\n",
        "      # print(RGB_X.shape)\n",
        "\n",
        "      X_NonAD.append(RGB_X)\n",
        "\n",
        "      if len(X_NonAD) % 200 == 0:\n",
        "        print(len(X_NonAD), \" of the 3200 have been uploaded\")\n",
        "  \n",
        "  X_test = np.concatenate((np.array(X_AD[:test_len]), np.array(X_NonAD[:test_len])))\n",
        "  y_test = np.concatenate((np.ones(test_len), np.zeros(test_len)))\n",
        "  \n",
        "  X_non_test_AD =  X_AD[test_len:]\n",
        "  X_non_test_nonAD =  X_NonAD[test_len:]\n",
        "\n",
        "  folds_X_train =[]\n",
        "  folds_y_train = []\n",
        "  \n",
        "  folds_X_val =[]\n",
        "  folds_y_val = []\n",
        "\n",
        "  for i in range(num_folds):\n",
        "    random.shuffle(X_non_test_AD)\n",
        "    random.shuffle(X_non_test_nonAD)\n",
        "\n",
        "    X_train = np.concatenate((np.array(X_non_test_AD[:train_len]), \n",
        "                              np.array(X_non_test_nonAD[:train_len])))\n",
        "    X_val = np.concatenate((np.array(X_non_test_AD[train_len:train_len+val_len]),\n",
        "                            np.array(X_non_test_nonAD[train_len:train_len+val_len])))\n",
        "    \n",
        "    folds_X_train.append(X_train)\n",
        "    folds_y_train.append(np.concatenate((np.ones(train_len), np.zeros(train_len))))\n",
        "\n",
        "    folds_X_val.append(X_val)\n",
        "    folds_y_val.append(np.concatenate((np.ones(val_len), np.zeros(val_len))))\n",
        "  \n",
        "  return X_test, y_test, folds_X_train, folds_y_train, folds_X_val, folds_y_val"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs0WVHrnKGYJ",
        "outputId": "1d0cab79-54cf-4277-c75d-895f3bec274d"
      },
      "source": [
        "X_test, y_test, folds_X_train, folds_y_train, folds_X_val, folds_y_val = get_data(3, len_AD_class = 3200)\n",
        "print(X_test.shape, y_test.shape, folds_X_train[0].shape, folds_y_train[0].shape,\n",
        "      folds_X_val[0].shape, folds_y_val[0].shape)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200  of the 3200 have been uploaded\n",
            "400  of the 3200 have been uploaded\n",
            "600  of the 3200 have been uploaded\n",
            "800  of the 3200 have been uploaded\n",
            "1000  of the 3200 have been uploaded\n",
            "1200  of the 3200 have been uploaded\n",
            "1400  of the 3200 have been uploaded\n",
            "1600  of the 3200 have been uploaded\n",
            "1800  of the 3200 have been uploaded\n",
            "2000  of the 3200 have been uploaded\n",
            "2200  of the 3200 have been uploaded\n",
            "2400  of the 3200 have been uploaded\n",
            "2600  of the 3200 have been uploaded\n",
            "2800  of the 3200 have been uploaded\n",
            "3000  of the 3200 have been uploaded\n",
            "3200  of the 3200 have been uploaded\n",
            "200  of the 3200 have been uploaded\n",
            "400  of the 3200 have been uploaded\n",
            "600  of the 3200 have been uploaded\n",
            "800  of the 3200 have been uploaded\n",
            "1000  of the 3200 have been uploaded\n",
            "1200  of the 3200 have been uploaded\n",
            "1400  of the 3200 have been uploaded\n",
            "1600  of the 3200 have been uploaded\n",
            "1800  of the 3200 have been uploaded\n",
            "2000  of the 3200 have been uploaded\n",
            "2200  of the 3200 have been uploaded\n",
            "2400  of the 3200 have been uploaded\n",
            "2600  of the 3200 have been uploaded\n",
            "2800  of the 3200 have been uploaded\n",
            "3000  of the 3200 have been uploaded\n",
            "3200  of the 3200 have been uploaded\n",
            "(1280, 224, 224, 3) (1280,) (4096, 224, 224, 3) (4096,) (1024, 224, 224, 3) (1024,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1hsgzvoOBCn"
      },
      "source": [
        "# Load ADNI data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2bE38irN-Vv"
      },
      "source": [
        "# dataset_path = Project_path + '/ADNI_data/'\n",
        "# X = np.load(dataset_path + '/Data.npy')\n",
        "# y = np.load(dataset_path + '/Labels.npy')\n",
        "# print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPU8BvbGlNnn"
      },
      "source": [
        "params_dict = {'Transfer': {'initial_weight':'imagenet', 'trainable_layer':False},\n",
        "               'Fine_tuning': {'initial_weight':'imagenet', 'trainable_layer':False},\n",
        "               'Baseline': {'initial_weight':None, 'trainable_layer':True} }"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CghoYlrErbHT"
      },
      "source": [
        "# Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY6cyBaJraR-"
      },
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNw6wSS4rdr8"
      },
      "source": [
        "# Model functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTSVrHn6HZYj"
      },
      "source": [
        "def create_model(model_kind, trainable_layers, add_type, dropouts, model_cat='Baseline', input_size=224):\n",
        "    set_trainable = False\n",
        "    \n",
        "    Model = Sequential()\n",
        "    Model.add(model_kind(include_top=False, pooling='avg', weights = params_dict[model_cat]['initial_weight'], input_shape=(input_size, input_size, 3)))\n",
        "\n",
        "    for layer in Model.layers:\n",
        "      if model_cat == 'Fine_tuning':\n",
        "         if layer.name in trainable_layers:\n",
        "          layer.trainable = False\n",
        "         else:\n",
        "          layer.trainable = True\n",
        "      else:\n",
        "        layer.trainable = params_dict[model_cat]['trainable_layer']\n",
        "      \n",
        "    Model.add(Flatten())\n",
        "    if add_type == 0:\n",
        "      Model.add(Dense(256, activation='relu'))\n",
        "      Model.add(Dropout(dropouts[0]))\n",
        "    elif add_type == 1:\n",
        "      Model.add(Dense(256, activation='relu'))\n",
        "      Model.add(Dropout(dropouts[0]))\n",
        "      Model.add(Dense(128, activation='relu'))\n",
        "      Model.add(Dropout(dropouts[1]))\n",
        "\n",
        "    # VGG.add(BatchNormalization())\n",
        "\n",
        "    # VGG.add(BatchNormalization())\n",
        "    # VGG.add(Dense(2048, activation='relu'))\n",
        "    # VGG.add(BatchNormalization())\n",
        "    Model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    print(Model.summary())\n",
        "    # Check the trainable status of the individual layers\n",
        "    for layer in Model.layers:\n",
        "        print(layer, layer.trainable)\n",
        "\n",
        "    opt = RMSprop(learning_rate = 0.001)\n",
        "    # opt = Adam(learning_rate=0.01)\n",
        "    Model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m])\n",
        "    return Model\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml5n-f5qVOxH"
      },
      "source": [
        "## ADNI Model_training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1UGtwT1HZbE"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=early_stopping_patience, restore_best_weights=True)\n",
        "\n",
        "for repeat in range(1, 6):\n",
        "    for model_kind in model_kinds:\n",
        "      for model_cat in model_cats:\n",
        "            if model_caat == 'Fine_tuning':\n",
        "              trainable_layers = []\n",
        "            else:\n",
        "              trainable_layers = []\n",
        "\n",
        "            fold_no = 1\n",
        "            Vacc_per_fold = []\n",
        "            Vloss_per_fold = []\n",
        "\n",
        "            # perform cross validation\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "            for train, val in kfold.split(X_train, y_train):\n",
        "\n",
        "                # csv_logger = CSVLogger(dataset_path + 'training_'+ str(model_name) + str(transfer_learning)+'r_'+str(repeat)+'_'+ str(fold_no)+'.log')\n",
        "\n",
        "                model = create_model(model_name, trainable_layers, model_cat=model_cat):\n",
        "                print(f'Training for fold {fold_no} ...', X_train[train].shape[0], X_train[val].shape[0])\n",
        "\n",
        "                history = model.fit(X_train[train], y_train[train],\n",
        "                                  batch_size=batch_num,\n",
        "                                  epochs=num_epochs,\n",
        "                                  verbose=1, validation_data=(X_train[val], y_train[val]),\n",
        "                                  callbacks=[early_stopping])\n",
        "\n",
        "                # Generate generalization metrics\n",
        "                scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "                print(scores)\n",
        "                print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1] * 100}%')\n",
        "\n",
        "                y_pred = model.predict(X_test, batch_size=64, verbose=1)\n",
        "                y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "                print(classification_report(y_test, y_pred_bool))\n",
        "\n",
        "                y_pred_reshaped = y_pred.reshape(y_pred.shape[0])\n",
        "                predictor = np.array([round(num) for num in y_pred_reshaped])\n",
        "\n",
        "                # print(prediction.shape, validation_labels.shape)\n",
        "                Confus = confusion_matrix(y_test, predictor)\n",
        "                print(Confus)\n",
        "\n",
        "\n",
        "                Vacc_per_fold.append(scores[1] * 100)\n",
        "                Vloss_per_fold.append(scores[0])\n",
        "\n",
        "                # Increase fold number\n",
        "                fold_no = fold_no + 1\n",
        "\n",
        "            print(Vacc_per_fold, Vloss_per_fold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKH0_m-ZVT-W"
      },
      "source": [
        "## OASIS model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFTo23tHMSDh",
        "outputId": "7421552d-615a-41de-86fe-4eb66f74073c"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=early_stopping_patience, restore_best_weights=True)\n",
        "\n",
        "fold_nums = 5\n",
        "num_repeats = 5\n",
        "num_epochs = 30\n",
        "add_types = [0]\n",
        "dropouts = [[0.5], [0.2]]\n",
        "model_kinds = [VGG16]\n",
        "model_cats =  ['Baseline']\n",
        "for repeat in range(1, num_repeats+1):\n",
        "    for model_name in model_kinds:\n",
        "      for model_cat in model_cats:\n",
        "        for add_type in add_types:\n",
        "          for dropout in dropouts:\n",
        "            \n",
        "            \n",
        "            Vacc_per_fold = []\n",
        "            Vloss_per_fold = []\n",
        "\n",
        "            if model_cat == 'Fine_tuning':\n",
        "              trainable_layers = []\n",
        "            else:\n",
        "              trainable_layers = []\n",
        "\n",
        "            for i in range(fold_nums):\n",
        "                print('repeat', repeat, 'fold', i)\n",
        "\n",
        "                model = create_model(model_name, trainable_layers, add_type, dropout, model_cat=model_cat)\n",
        "\n",
        "                history = model.fit(folds_X_train[i], folds_y_train[i], batch_size=batch_num,\n",
        "                                  epochs=num_epochs,\n",
        "                                  verbose=1,\n",
        "                                  validation_data=(folds_X_val[i], folds_y_val[i]), \n",
        "                                   callbacks=[early_stopping])     \n",
        "\n",
        "                # Generate generalization metrics\n",
        "                scores = model.evaluate(X_test, verbose=0)\n",
        "                print(scores)\n",
        "                print(f'Score for fold {i}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1] * 100}%')\n",
        "\n",
        "                y_pred = model.predict(X_test, batch_size=64, verbose=1)\n",
        "                y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "                print(classification_report(y_test, y_pred_bool))\n",
        "\n",
        "                y_pred_reshaped = y_pred.reshape(y_pred.shape[0])\n",
        "                predictor = np.array([round(num) for num in y_pred_reshaped])\n",
        "\n",
        "                # print(prediction.shape, validation_labels.shape)\n",
        "                Confus = confusion_matrix(y_test, predictor)\n",
        "                print(Confus)\n",
        "\n",
        "\n",
        "                Vacc_per_fold.append(scores[1] * 100)\n",
        "                Vloss_per_fold.append(scores[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "repeat 1 fold 0\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 512)               14714688  \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 14,846,273\n",
            "Trainable params: 14,846,273\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "<tensorflow.python.keras.engine.functional.Functional object at 0x7f558f1b5190> True\n",
            "<tensorflow.python.keras.layers.core.Flatten object at 0x7f558c37bf10> True\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f558c3a8f50> True\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f558c3b0150> True\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f558c337fd0> True\n",
            "Epoch 1/30\n",
            "137/137 [==============================] - 30s 208ms/step - loss: 8959.2109 - accuracy: 0.4978 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 2/30\n",
            "137/137 [==============================] - 28s 207ms/step - loss: 0.7034 - accuracy: 0.4976 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "137/137 [==============================] - 28s 207ms/step - loss: 0.6934 - accuracy: 0.4951 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "137/137 [==============================] - 28s 207ms/step - loss: 0.6933 - accuracy: 0.4983 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "104/137 [=====================>........] - ETA: 6s - loss: 0.6932 - accuracy: 0.5038"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvRCKGTyhIVe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FnPzn_xcuRV"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8iYnsZMcvx3"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=early_stopping_patience, restore_best_weights=True)\n",
        "\n",
        "nb_train_samples = 5120\n",
        "nb_validation_samples = 1280\n",
        "fold_no = 0\n",
        "fold_nums = 1\n",
        "num_repeats = 1\n",
        "num_epochs = 2\n",
        "model_kinds = [VGG16]\n",
        "model_cats =  ['Baseline']\n",
        "for repeat in range(1, num_repeats+1):\n",
        "    for model_name in model_kinds:\n",
        "      for model_cat in model_cats:\n",
        "            Vacc_per_fold = []\n",
        "            Vloss_per_fold = []\n",
        "            fold_no = 0\n",
        "\n",
        "            if model_cat == 'Fine_tuning':\n",
        "              trainable_layers = []\n",
        "            else:\n",
        "              trainable_layers = []\n",
        "\n",
        "\n",
        "            for i in range(fold_nums):\n",
        "                train_data_dir = data_path + '/data' + str(i) + '/train/'\n",
        "                validation_data_dir = data_path + '/data' + str(i) + '/validation'\n",
        "\n",
        "                train_labels = np.array(\n",
        "                        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
        "                \n",
        "                validation_labels = np.array(\n",
        "                    [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
        "                \n",
        "                datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "                train_generator = datagen.flow_from_directory(\n",
        "                    train_data_dir,\n",
        "                    target_size=(img_width, img_height),\n",
        "                    batch_size=batch_num,\n",
        "                    class_mode='binary',\n",
        "                    shuffle=False)\n",
        "                val_generator = datagen.flow_from_directory(\n",
        "                    validation_data_dir,\n",
        "                    target_size=(img_width, img_height),\n",
        "                    batch_size=batch_num,\n",
        "                    class_mode='binary',\n",
        "                    shuffle=False)\n",
        "                print('Data created')\n",
        "\n",
        "                model = create_model(model_name, trainable_layers, model_cat=model_cat)\n",
        "                print(f'Training for fold {fold_no} ...')\n",
        "                history = model.fit(train_generator, epochs=num_epochs,steps_per_epoch=10,\n",
        "                                              validation_data=val_generator, validation_steps=10, \n",
        "                                              verbose=1, callbacks=[early_stopping])     \n",
        "\n",
        "                # Generate generalization metrics\n",
        "                scores = model.evaluate(val_generator, verbose=0)\n",
        "                print(scores)\n",
        "                print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1] * 100}%')\n",
        "\n",
        "                y_pred = model.predict(val_generator, batch_size=64, verbose=1)\n",
        "                y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "                print(classification_report(validation_labels, y_pred_bool))\n",
        "\n",
        "                y_pred_reshaped = y_pred.reshape(y_pred.shape[0])\n",
        "                predictor = np.array([round(num) for num in y_pred_reshaped])\n",
        "\n",
        "                # print(prediction.shape, validation_labels.shape)\n",
        "                Confus = confusion_matrix(validation_labels, predictor)\n",
        "                print(Confus)\n",
        "\n",
        "\n",
        "                Vacc_per_fold.append(scores[1] * 100)\n",
        "                Vloss_per_fold.append(scores[0])\n",
        "\n",
        "                # Increase fold number\n",
        "                fold_no =+1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3J88TPFdlO1"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def get_data_files(AD_files, non_AD_files, kfold):\n",
        "\n",
        "\n",
        "  fold_train_data = []\n",
        "  fold_val_data = []\n",
        "\n",
        "  train_len =  int(0.64 * len(AD_files))\n",
        "  val_len =  int(0.16 * len(AD_files))\n",
        "  test_len = int(0.20 * len(AD_files))\n",
        "\n",
        "  print(len(AD_files), len(non_AD_files))\n",
        "\n",
        "  # AD_test = np.random.choice(AD_files, size=test_len, replace=False)\n",
        "  # NonAD_test = np.random.choice(non_AD_files, size=test_len, replace=False)\n",
        "  random.shuffle(AD_files)\n",
        "  random.shuffle(non_AD_files)\n",
        "\n",
        "  AD_test = AD_files[:test_len]\n",
        "  NonAD_test = non_AD_files[:test_len]\n",
        "\n",
        "  test_set = [AD_test, NonAD_test]\n",
        "  \n",
        "  AD_files_after_test_indexes = AD_files[test_len:]\n",
        "  NonAD_files_after_test_indexes =  non_AD_files[test_len:]\n",
        "\n",
        "  print(len(AD_files_after_test_indexes), len(NonAD_files_after_test_indexes))\n",
        "\n",
        "\n",
        "  for i in range(kfold):\n",
        "\n",
        "    random.shuffle(AD_files_after_test_indexes)\n",
        "    random.shuffle(NonAD_files_after_test_indexes)\n",
        "\n",
        "    AD_train = AD_files_after_test_indexes[:train_len]\n",
        "    NonAD_train = NonAD_files_after_test_indexes[:train_len]\n",
        "\n",
        "    AD_val = AD_files_after_test_indexes[train_len: ]\n",
        "    NonAD_val = NonAD_files_after_test_indexes[train_len:]\n",
        "  \n",
        "    fold_train_data.append([AD_train, NonAD_train])\n",
        "    fold_val_data.append([AD_val, NonAD_val])\n",
        "\n",
        "    print('AD datasets:', len(AD_train), len(AD_val), len(AD_test))\n",
        "    print('Non_AD datasets:', len(NonAD_train), len(NonAD_val), len(NonAD_test))\n",
        "\n",
        "  return fold_train_data, fold_val_data , test_set\n",
        "\n",
        "get_data_files(Alzheimers_files, NonAlzheimers_files, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYMR0ohwdqJx"
      },
      "source": [
        "\n",
        "# def get_data(batch_num, len_AD_class = 3200)\n",
        "    \n",
        "\n",
        "#     train_len =  int(0.64 * len_AD_class//batch_num)\n",
        "#     val_len =  int(0.16 * len_AD_class//batch_num)\n",
        "#     test_len = int(0.20 * len_AD_class//batch_num)\n",
        "\n",
        "#     train_data_AD = image_dataset_from_directory(\n",
        "#     Project_path + '/Sanaz_Code/OASIS_data/New/Alzheimers/', labels=None, shuffle=True)\n",
        "\n",
        "#     train_data_NonAD = image_dataset_from_directory(\n",
        "#     Project_path + '/Sanaz_Code/OASIS_data/New/NonAlzheimer/', labels=None, shuffle=True)\n",
        "    \n",
        "#     for x, y in train_data:\n",
        "#       print(x.shape)\n",
        "#       ds = a.concatenate(b)\n",
        "\n",
        "#       break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}